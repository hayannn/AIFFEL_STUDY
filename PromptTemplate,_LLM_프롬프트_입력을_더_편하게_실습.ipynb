{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hayannn/AIFFEL_STUDY/blob/main/PromptTemplate%2C_LLM_%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8_%EC%9E%85%EB%A0%A5%EC%9D%84_%EB%8D%94_%ED%8E%B8%ED%95%98%EA%B2%8C_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실습 준비"
      ],
      "metadata": {
        "id": "HnoSfwZqwKbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai langchain langchain-openai"
      ],
      "metadata": {
        "id": "EBFyoFPb-_7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "fJTbFNJ_uB3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Qhh9fyI8-d9v"
      },
      "outputs": [],
      "source": [
        "#API KEY 저장을 위한 os 라이브러리 호출\n",
        "import os\n",
        "\n",
        "#기본 LLM 로드를 위한 라이브러리 호출\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "#채팅 LLM 로드를 위한 라이브러리 호출\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_community.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OPENAI API키 저장\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'YOUR_API_KEY'"
      ],
      "metadata": {
        "id": "9F6f494n-IWA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------"
      ],
      "metadata": {
        "id": "DczHm9qI994V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Davinch-003 모델 설정하기 ➡️ gpt-3.5-turbo로 교체"
      ],
      "metadata": {
        "id": "THkva8fB-C0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "davinci3 = OpenAI(\n",
        "    model_name=\"text-davinci-003\",\n",
        "    max_tokens = 1000\n",
        ")"
      ],
      "metadata": {
        "id": "uqur0akRHi0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- deprecated 상태의 모델이기 때문에 gpt-3.5-turbo로 모델 교체"
      ],
      "metadata": {
        "id": "uffHPrzqzZNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3 = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    max_tokens=1000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMrI7dWyzJUG",
        "outputId": "86e3c41e-a66f-4f3f-bbea-59c69de6aa1d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-a269f493682e>:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  gpt3 = ChatOpenAI(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "4p3ySI4Xu6ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 프롬프트 템플릿 맛보기"
      ],
      "metadata": {
        "id": "QhCfB_lQ-uqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "프롬프트 템플릿은 크게 2가지가 존재합니다.\n",
        "1. Prompt Template\n",
        "2. Chat Prompt Template\n",
        "\n",
        "1번 Prompt Template은 일반적인 프롬프트 템플릿을 생성할때 활용합니다.\n",
        "\n",
        "2번 Chat Prompt Template은 채팅 LLM에 프롬프트를 전달하는 데에 활용할 수 있는 특화 프롬프트 템플릿입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "LrfEmGR5Ctbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt Template"
      ],
      "metadata": {
        "id": "Q8-1Q_eUvqTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "\n",
        "#프롬프트 템플릿을 통해 매개변수 삽입 가능한 문자열로 변환\n",
        "string_prompt = PromptTemplate.from_template(\"tell me a joke about {subject}\")\n",
        "\n",
        "#매개변수 삽입한 결과를 string_prompt_value에 할당\n",
        "string_prompt_value = string_prompt.format_prompt(subject=\"soccer\")\n",
        "\n",
        "#채팅LLM이 아닌 LLM과 대화할 때 필요한 프롬프트 = string prompt\n",
        "string_prompt_value"
      ],
      "metadata": {
        "id": "-bX3Wvsy-tyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d3d7a3-9a6e-4114-fcdc-d91cd05f08bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='tell me a joke about soccer')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- to_string() 함수를 통해 prompt template으로 생성한 문장 raw_text 반환 가능"
      ],
      "metadata": {
        "id": "bagZnlH1vdJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(string_prompt_value.to_string())"
      ],
      "metadata": {
        "id": "6F3GCMX7-7j8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2394a369-dd16-4e78-b713-2bcd339e41b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tell me a joke about soccer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chat Prompt Template"
      ],
      "metadata": {
        "id": "wpLkkpG_vshg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_template(\"tell me a joke about {subject}\")\n",
        "chat_prompt_value = chat_prompt.format_prompt(subject=\"soccer\")\n",
        "chat_prompt_value"
      ],
      "metadata": {
        "id": "vylmSAk2_dfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2270ebd4-8888-4847-bbb8-aee13d931fbb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='tell me a joke about soccer', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt_value.to_string()"
      ],
      "metadata": {
        "id": "47q4Uy2y_bM8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "968d1a31-5df2-4be3-d9e5-e0cc829964a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: tell me a joke about soccer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "Md2LlE_PwICI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 프롬프트 템플릿 활용해보기"
      ],
      "metadata": {
        "id": "jVF-pxeUE4JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "반복적인 프롬프트를 삽입해야하는 경우, Prompt Template를 통해 간편하게 LLM을 활용할 수 있습니다."
      ],
      "metadata": {
        "id": "RIoNBQ_HFNlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GPT-3와 프롬프트 템플릿을 활용하여 대화해보기"
      ],
      "metadata": {
        "id": "25dmtYfJajHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "너는 요리사야. 내가 가진 재료들을 갖고 만들 수 있는 요리를 추천하고, 그 요리의 레시피를 제시해줘.\n",
        "내가 가진 재료는 아래와 같아.\n",
        "\n",
        "<재료>\n",
        "{재료}\n",
        "\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables = ['재료'],\n",
        "    template = template\n",
        ")"
      ],
      "metadata": {
        "id": "p1urKI18_bxI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_template.format(재료 = '양파, 계란, 사과, 빵'))"
      ],
      "metadata": {
        "id": "lhBbXyEWGGE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c49602c-dbd2-42b8-c30c-fa0081173942"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "너는 요리사야. 내가 가진 재료들을 갖고 만들 수 있는 요리를 추천하고, 그 요리의 레시피를 제시해줘.\n",
            "내가 가진 재료는 아래와 같아.\n",
            "\n",
            "<재료>\n",
            "양파, 계란, 사과, 빵\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = gpt3.predict(\n",
        "    prompt_template.format(재료=\"양파, 계란, 사과, 빵\")\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "nSxX_4auG4UM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcc074d-25cb-43bd-d227-5987fe2e10fa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-c6f8c8e6f34e>:1: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = gpt3.predict(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "제가 추천하는 요리는 \"양파 계란 볶음밥\"입니다. \n",
            "\n",
            "<양파 계란 볶음밥 레시피>\n",
            "1. 양파를 다져주세요.\n",
            "2. 팬에 식용유를 두르고 양파를 볶다가 계란을 풀어 넣어 볶아주세요.\n",
            "3. 밥을 넣고 골고루 섞어주세요.\n",
            "4. 간장, 소금, 후추로 간을 해주세요.\n",
            "5. 사과를 곁들여 먹으면 더 맛있어요.\n",
            "6. 빵과 함께 서빙해주세요.\n",
            "\n",
            "이렇게 간단하면서도 맛있는 요리를 즐기세요!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT와 프롬프트 템플릿을 활용하여 대화해보기"
      ],
      "metadata": {
        "id": "Ea5k-T2KacpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ],
      "metadata": {
        "id": "9yRP_2-aR9yr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatGPT 모델 로드\n",
        "chatgpt = ChatOpenAI(temperature=0)\n",
        "\n",
        "# 역할 부여(위에서 정의한 Template 사용)\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "# 사용자가 입력할 매개변수 template 선언\n",
        "human_template = \"{재료}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "#ChatPromptTemplate에 system message와 human message 템플릿 삽입\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# ChatGPT API에 ChatPromptTemplate 입력 시, human message의 매개변수인 '재료'를 할당해 전달\n",
        "# -> ChatGPT는 ChatPromptTemplate의 system message와 human message를 전달받아 -> 대답 생성에 활용\n",
        "answer = chatgpt(chat_prompt.format_prompt(재료=\"양파, 계란, 사과, 빵\").to_messages())\n",
        "print(answer.content)"
      ],
      "metadata": {
        "id": "7jgLtLMPHKI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27484817-200a-412d-efc7-f72386405060"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-b7fac58ea93d>:16: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  answer = chatgpt(chat_prompt.format_prompt(재료=\"양파, 계란, 사과, 빵\").to_messages())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "제가 추천하는 요리는 \"양파 계란 토스트\"입니다. 이 요리는 간단하면서도 맛있는 아침 식사나 간식으로 딱 좋아요. 준비물은 양파, 계란, 빵이 필요합니다.\n",
            "\n",
            "**양파 계란 토스트 레시피:**\n",
            "\n",
            "**재료:**\n",
            "- 빵 4조각\n",
            "- 양파 1개\n",
            "- 계란 2개\n",
            "- 소금\n",
            "- 후추\n",
            "- 식용유\n",
            "\n",
            "**만드는 법:**\n",
            "1. 양파를 얇게 썰어줍니다.\n",
            "2. 팬에 식용유를 두르고 양파를 볶아 투명해질 때까지 볶아줍니다.\n",
            "3. 볶은 양파를 한쪽으로 밀어 팬을 비워줍니다.\n",
            "4. 계란을 푼 후 소금과 후추로 간을 해줍니다.\n",
            "5. 양파가 있는 쪽에 계란을 부어줍니다.\n",
            "6. 계란이 익을 때까지 젓가락으로 저어가며 볶아줍니다.\n",
            "7. 빵을 토스터기에 굽거나 팬에 구워줍니다.\n",
            "8. 계란이 익은 후 빵 위에 올려줍니다.\n",
            "9. 다른 빵 조각으로 덮어 완성합니다.\n",
            "\n",
            "맛있는 양파 계란 토스트가 완성되었습니다. 맛있게 드세요!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "KYtZ0A2u2JmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Few-shot 예제를 통한 프롬프트 템플릿"
      ],
      "metadata": {
        "id": "LSEhdhkRazTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 딥러닝 모델이 결과물을 출력할 때 예시 결과물을 제시함으로써 원하는 결과물로 유도하는 방법론\n",
        "- LLM에 Few-shot 예제를 제공하면 예제와 유사한 형태의 결과물을 출력함\n",
        "- 내가 원하는 결과물의 형태가 특수하거나, 구조화된 답변을 원할 경우 사용\n",
        "- 결과물의 예시를 여러 개 제시 ➡️ 결과물의 품질 향상 가능"
      ],
      "metadata": {
        "id": "oXkGkd_Xa5o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "examples = [\n",
        "  {\n",
        "    \"question\": \"아이유로 삼행시 만들어줘\",\n",
        "    \"answer\":\n",
        "\"\"\"\n",
        "아: 아이유는\n",
        "이: 이런 강의를 들을 이\n",
        "유: 유가 없다.\n",
        "\"\"\"\n",
        "  },\n",
        "\n",
        "  {\n",
        "    \"question\": \"김민수로 삼행시 만들어줘\",\n",
        "    \"answer\":\n",
        "\"\"\"\n",
        "김: 김치는 맛있다\n",
        "민: 민달팽이도 좋아하는 김치!\n",
        "수: 수억을 줘도 김치는 내꺼!\n",
        "\"\"\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "P71OBMpBVPY5"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")\n",
        "\n",
        "print(example_prompt.format(**examples[0]))"
      ],
      "metadata": {
        "id": "vYoqJvxlbbLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17646751-f44b-41fc-d445-785366f3afd6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: 아이유로 삼행시 만들어줘\n",
            "\n",
            "아: 아이유는\n",
            "이: 이런 강의를 들을 이\n",
            "유: 유가 없다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {input}\",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "print(prompt.format(input=\"호날두로 삼행시 만들어줘\"))"
      ],
      "metadata": {
        "id": "Ip1L1Kvmbj6p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edaa1f1-6078-4949-b802-7f0e2a15b97b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: 아이유로 삼행시 만들어줘\n",
            "\n",
            "아: 아이유는\n",
            "이: 이런 강의를 들을 이\n",
            "유: 유가 없다.\n",
            "\n",
            "\n",
            "Question: 김민수로 삼행시 만들어줘\n",
            "\n",
            "김: 김치는 맛있다\n",
            "민: 민달팽이도 좋아하는 김치!\n",
            "수: 수억을 줘도 김치는 내꺼!\n",
            "\n",
            "\n",
            "Question: 호날두로 삼행시 만들어줘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gpt3.predict(\"호날두로 삼행시 만들어줘\"))"
      ],
      "metadata": {
        "id": "ombnB2752_XU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056f2a32-a052-4de9-fa9f-be0d4ba77ac6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "호날두는 축구 신이요\n",
            "공을 다루는 그 모습이\n",
            "팬들을 미치게 만들어\n",
            "\n",
            "호날두는 레전드 선수\n",
            "기술과 스피드 모두 뛰어\n",
            "경기장을 빛내주네\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt3 = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"호날두로 삼행시 만들어줘\"}\n",
        "]\n",
        "\n",
        "response = gpt3(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuLitz_V6mq1",
        "outputId": "92f00f82-6368-4461-ee69-23cf5e40375b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='호날두는 축구왕,\\n공을 다루는 손재주,\\n승리는 그의 몫이야.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 24, 'total_tokens': 59, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-9fb785e7-b3f9-4504-a301-a1449503daa6-0' usage_metadata={'input_tokens': 24, 'output_tokens': 35, 'total_tokens': 59, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(gpt3(\n",
        "#     prompt.format(input=\"호날두로 삼행시 만들어줘\")\n",
        "# ))"
      ],
      "metadata": {
        "id": "kOwffrVsdADU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")\n",
        "\n",
        "# prompt = FewShotPromptTemplate(\n",
        "#     examples=examples,\n",
        "#     example_prompt=example_prompt,\n",
        "#     suffix=\"Question: {input}\",\n",
        "#     input_variables=[\"input\"]\n",
        "# )\n",
        "\n",
        "# gpt3 = ChatOpenAI(\n",
        "#     model_name=\"gpt-3.5-turbo\",\n",
        "#     max_tokens=1000\n",
        "# )\n",
        "\n",
        "formatted_prompt = prompt.format(input=\"호날두로 삼행시 만들어줘\")\n",
        "response = gpt3([{\"role\": \"user\", \"content\": formatted_prompt}])\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwt_OyaV7cfA",
        "outputId": "18c84d7f-258b-4035-e729-4356cbc328f1"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "호: 호날두는\n",
            "날: 날마다 골을 넣는다\n",
            "두: 두 손으로 팀을 이끈다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Example Selector를 이용한 동적 Few-shot 러닝"
      ],
      "metadata": {
        "id": "9bzF-tKHiw3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Example Selector` 활용\n",
        "- LLM이 여러 작업을 수행하면서도 **원하는 범위의 대답을 출력**하도록 하려면..\n",
        "  - 사용자의 입력에 동적으로 반응해야함\n",
        "  - 이와 동시에, 예제를 모두 학습시키는 것이 아니라 적절한 예시만 포함하도록 함으로써 입력 prompt의 길이를 제한하고, 이를 통해 오류가 발생하지 않도록 조절할 수 있음"
      ],
      "metadata": {
        "id": "SFriPzdEi-6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `SemanticSimilarityExampleSelector`\n",
        "  - 사용자 입력과 예제 사이의 유사성 비교 ➡️ 사용자 입력과 비슷한 예제부터 가져오도록 하는 것"
      ],
      "metadata": {
        "id": "BJzvemza9ZIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "# These are a lot of examples of a pretend task of creating antonyms.\n",
        "examples = [\n",
        "    # 감정 비교\n",
        "    {\"input\": \"행복\", \"output\": \"슬픔\"},\n",
        "    {\"input\": \"흥미\", \"output\": \"지루\"},\n",
        "    {\"input\": \"불안\", \"output\": \"안정\"},\n",
        "\n",
        "    # 대소 단위 비교\n",
        "    {\"input\": \"긴 기차\", \"output\": \"짧은 기차\"},\n",
        "    {\"input\": \"큰 공\", \"output\": \"작은 공\"},\n",
        "]"
      ],
      "metadata": {
        "id": "ImuHUY24i7E8"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "hZdsaKvSk2hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 주어진 예시들을 바탕으로 반대 의미를 가진 단어를 출력하는 템플릿 생성\n",
        "  - `example_selector` : 입력과 유사한 예시를 찾기 위해 OpenAI 임베딩과 Chroma를 사용하여 가장 비슷한 예시를 선택\n",
        "  - `FewShotPromptTemplate` : 선택된 예시를 바탕으로, \"반대 의미를 가진 단어\"를 출력하는 프롬프트를 생성\n",
        "  - 입력 : 사용자가 제공한 단어에 대해 모델이 반대 의미를 출력하도록 유도"
      ],
      "metadata": {
        "id": "xE-a1GlL-SS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples,\n",
        "    OpenAIEmbeddings(),\n",
        "    Chroma,\n",
        "    k=1\n",
        ")\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"주어진 입력에 대해 반대의 의미를 가진 단어를 출력해줘\",\n",
        "    suffix=\"Input: {단어}\\nOutput:\",\n",
        "    input_variables=[\"단어\"],\n",
        ")"
      ],
      "metadata": {
        "id": "7fR63b8OkkT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2729e5-cf28-4af1-8d3a-e4b557adfa7d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-ea5bcf3d34df>:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  OpenAIEmbeddings(),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input is a feeling, so should select the happy/sad example\n",
        "print(similar_prompt.format(단어=\"무서운\"))"
      ],
      "metadata": {
        "id": "IBLqJJ1Glmim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f28994-a541-467b-965c-f3ed246f809a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "주어진 입력에 대해 반대의 의미를 가진 단어를 출력해줘\n",
            "\n",
            "Input: 불안\n",
            "Output: 안정\n",
            "\n",
            "Input: 무서운\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input is a feeling, so should select the happy/sad example\n",
        "print(similar_prompt.format(단어=\"큰 비행기\"))"
      ],
      "metadata": {
        "id": "4PMPa7oAk1P5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d2d0c4-7b58-4ad7-fba5-4c0269e00335"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "주어진 입력에 대해 반대의 의미를 가진 단어를 출력해줘\n",
            "\n",
            "Input: 긴 기차\n",
            "Output: 짧은 기차\n",
            "\n",
            "Input: 큰 비행기\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"큰 비행기\"\n",
        "\n",
        "print(gpt3(\n",
        "    similar_prompt.format(단어=query)\n",
        "))"
      ],
      "metadata": {
        "id": "QVhHxGDIl9vO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba304564-476f-4c20-d16e-2e878e212f55"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='작은 비행기' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 60, 'total_tokens': 67, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-4dbc9b94-6e52-4ecd-a471-af9eee5d0d96-0' usage_metadata={'input_tokens': 60, 'output_tokens': 7, 'total_tokens': 67, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "R_mPAJvB_aMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Output Parser를 활용한 출력값 조정"
      ],
      "metadata": {
        "id": "voHpTEbCtZbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `OutputParser`\n",
        "  - LLM의 답변을 내가 원하는 형태로 고정\n",
        "  - 리스트, JSON 형태 등 다양한 형식의 답변을 고정하여 출력 가능"
      ],
      "metadata": {
        "id": "OeOagtrYtddg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "m3kZmxYPndZE"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = CommaSeparatedListOutputParser()"
      ],
      "metadata": {
        "id": "WCv9v_o2tpns"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ],
      "metadata": {
        "id": "YszrHErKuGSr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ae37309c-14ae-4e66-9509-9067d0de38f5"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"{주제} 5개를 추천해줘.\\n{format_instructions}\",\n",
        "    input_variables=[\"주제\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ],
      "metadata": {
        "id": "9DwQ7cWptq5R"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "n2o_FERKtr7p"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_input = prompt.format(주제=\"영화\")\n",
        "output = model(_input)"
      ],
      "metadata": {
        "id": "JogXFdLrttG2"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Z0nSEvFhE4IS",
        "outputId": "30a21746-9dd9-455a-c45b-bf98d11cc067"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n1. Parasite\\n2. The Shawshank Redemption\\n3. Inception\\n4. The Godfather\\n5. Pulp Fiction'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser.parse(output)"
      ],
      "metadata": {
        "id": "YJSP5zCztuPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d877de16-9fe4-46ab-c861-970234a468d9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. Parasite',\n",
              " '2. The Shawshank Redemption',\n",
              " '3. Inception',\n",
              " '4. The Godfather',\n",
              " '5. Pulp Fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    }
  ]
}